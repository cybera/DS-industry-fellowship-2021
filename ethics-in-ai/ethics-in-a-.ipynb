{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align='center'>Ethics in Artificial Intelligence</h1>\n",
    "\n",
    "<h3 align='center'>Laura G. Funderburk</h3>\n",
    "\n",
    "<h3 align='center'>Data Scientist, Cybera</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>What is Artificial Intelligence (AI)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>What is Ethics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Why Ethics in AI matter</h2>\n",
    "\n",
    "\n",
    " AI systems can behave unfairly for a variety of reasons: \n",
    "\n",
    "1. Societal biases are reflected in the training data. \n",
    "\n",
    "2. Societal biases are reflected and in the decisions made during the development and deployment of these systems. \n",
    "\n",
    "3. AI systems behave unfairly because of characteristics of the data or characteristics of the systems themselves. \n",
    "\n",
    "$$\\Rightarrow \\text{They are not mutually exclusive and often exacerbate one another} \\Leftarrow$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>How can we determine whether an AI is behaving unfairly?</h2>\n",
    "\n",
    "\n",
    "1. Through identifying underlying misconceptions within AI - (causal: societal-based contenxt/bias, or in terms of intent, such as prejudice)\n",
    "2. Through study of impact of AI on people - (outcome: harms vs gains)\n",
    "\n",
    "**For this workshop, we define whether an AI system is behaving unfairly in terms of its impact on people.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>A note on the word bias</h2>\n",
    "\n",
    "Since we define fairness in terms of **harms** rather than specific **causes** (such as societal-based context), we avoid the usage of the words *bias* or *debiasing*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Types of harms</h2>\n",
    "\n",
    "From keynote by K. Crawford at NeurIPS 2017 <https://www.youtube.com/watch?v=fMym_BKWQzk>\n",
    "\n",
    "* *Allocation harms* can occur when AI systems extend or withhold\n",
    "  opportunities, resources, or information. \n",
    "  \n",
    "  **Sample key applications: hiring, school admissions, and lending.**\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Types of harms</h2>\n",
    "\n",
    "From keynote by K. Crawford at NeurIPS 2017 <https://www.youtube.com/watch?v=fMym_BKWQzk>\n",
    "\n",
    "\n",
    "* *Quality-of-service harms* can occur when a system does not work as well for\n",
    "  one person as it does for another, even if no opportunities, resources, or\n",
    "  information are extended or withheld. \n",
    "  \n",
    "  **Sample key applications: accuracy in face recognition, document search, or product recommendation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>How we work with information: abstraction</h2>\n",
    "\n",
    "\n",
    "**Abstracting in computer science:**  \n",
    "\n",
    "This is the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems with the goal of focusing attention on details deemed more important. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we work with information: abstraction</h2>\n",
    "\n",
    "\n",
    "**Abstracting in mathematics:** \n",
    "\n",
    "This is the process of \n",
    "\n",
    "1. Extracting the underlying structures, patterns or properties of a mathematical concept;\n",
    "\n",
    "2. Removing any dependence on real world objects with which it might originally have been connected;\n",
    "\n",
    "3. Generalizing it so that it has wider applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we work with information: abstraction</h2>\n",
    "\n",
    "\n",
    "**Abstracting in machine learning:** \n",
    "\n",
    "Machine Learning encompasses all approaches (design and development of algorithms) that allow a computer to “learn”, based on a database of examples or sensor data (abstractions of real situations).\n",
    "\n",
    "In Machine Learning abstraction manifests in algorithms learning from example (supervised/unsupervised learning), and learning from reinforcement (reinforcement learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we work with information: abstraction</h2>\n",
    "\n",
    "\n",
    "**Abstracting in machine learning:** \n",
    "\n",
    "Machine Learning algorithms can be classified into three broad classes [1]\n",
    "\n",
    "1. In supervised learning, an algorithm learns a function that maps inputs to a given set of labels (classes).\n",
    "\n",
    "2. In unsupervised learning, an algorithm learns how to unravel hidden structures in unlabeled data (also called observations).\n",
    "\n",
    "3. In reinforcement learning, an algorithm learns how an agent ought to take actions in an environment so as to maximize some reward.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How do we fall into misrepresentations in machine learning?</h2>\n",
    "\n",
    "\n",
    "$\\Rightarrow$ When we abstract and remove attributes or properties that have dependence on a social context, how do we determine which properties are *worth* preserving and describing?\n",
    "\n",
    "$\\Rightarrow$ What is the trade off we make when discarding a property, and are we aware of the consequences of that trade off?\n",
    "\n",
    "$\\Rightarrow$ Can we guarantee capability to identify and quantify the consequences of removing a social-based attribute? Who is on the receiving end of these consequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The algorithmic frame</h3>\n",
    "\n",
    "Frame centered around choices made when abstracting a problem in the form of representations (input data) and labelling (outcome). \n",
    "\n",
    "Evaluated on accuracy and generalizability to data the model did not train on. \n",
    "\n",
    "**Fairness cannot be defined in this frame $\\Rightarrow$ goal is to produce a model that best captures the relationship between representations and labels.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The data frame</h3>\n",
    "\n",
    "This frame is concerned with the quality of representations (input data) and the outcomes that result. \n",
    "\n",
    "**This additional frame allows us to question the inherent (un)fairness present in input and output data.**\n",
    "\n",
    "For example, under this frame we can question whether our training dataset incorporates demographic and socio-economic information related to an algorithm providing recommendations, and assesses the quality of those recommendations based on demographic and socio-economic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The sociotechnical frame</h3>\n",
    "\n",
    "$$\\text{Technical Component} + \\text{Social Component} = \\text{Sociotechnical system}$$\n",
    "\n",
    "\n",
    "This frame recognizes that a machine learning model is part of the interaction between people and technology, and thus any social components of this interaction need to be modelled and incorporated accordingly.\n",
    "\n",
    "**Designers of machine learning systems who fail to consider the way in which social contexts and technologies\n",
    "are interrelated are at risk of falling into \"abstraction traps\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assessing risk of re-engagement in criminal behaviour in an individual\n",
    "charged with an offense, and appropriate measures to prevent relapse, and failing\n",
    "to consider factors such as race, socio-economic status, mental health, along with\n",
    "socially-dependent views present in judges, police officers, or any actors responsible\n",
    "for recommending a course of action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the algorithmic framework, for example, input variables may contain previous criminal history,\n",
    "statements taken by the accused, witnesses and police officers. Labels (outcome)\n",
    "include recommendations by the algorithm on an appropriate course of action based\n",
    "on a computed risk score. Model is limited in assessing fairness out outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data framework could attempt to reduce unfairness by studying socio-economic\n",
    "information regarding the accused, their upbringing and how it relates to their\n",
    "current status, along with a recommendation that incorporates these factors into their\n",
    "recovery.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Within the sociotechnical framework the model incorporates not only more nuanced\n",
    "data on the history of the case, but also the social context in which judging and\n",
    "charging people with offenses take place. This model incorporates the processes\n",
    "associated with crime reporting, the offense-trial pipeline, and identifies areas\n",
    "in which different people interact with one another as outcomes are recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Common \"traps\" we can fall into</h2>\n",
    "\n",
    "Selbst *et al.* [2] identify five traps we can fall into when implementing a machine learning model:\n",
    "\n",
    "* The Solutionism Trap\n",
    "\n",
    "* The Ripple Effect Trap\n",
    "\n",
    "* The Formalism Trap\n",
    "\n",
    "* The Portability Trap\n",
    "\n",
    "* The Framing Trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common \"traps\" we can fall into</h2>\n",
    "<h3 align='center'>The Solutionism Trap</h3>\n",
    "\n",
    "\n",
    "This trap occurs when we assume that the best solution to a problem\n",
    "involves technology, and fail to recognize other possible solutions outside of\n",
    "this realm. One area where this manifests in is within contexts in which the\n",
    "definition \"fairness\" changes or is dependent on a political context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common \"traps\" we can fall into</h2>\n",
    "<h3 align='center'>The Ripple Effect Trap</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common \"traps\" we can fall into</h2>\n",
    "<h3 align='center'>The Formalism Trap</h3>\n",
    "\n",
    "This trap occurs when implementing an algorithmic solution that fails to take\n",
    "into account the social dimensions associated to fairness in a situation. These\n",
    "dimensions include \"procedurality\", \"contextuality\" and \"contestability\". Such\n",
    "dimensions can often not be resolved via a purely mathematical framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common \"traps\" we can fall into</h2>\n",
    "<h3 align='center'>The Portability Trap</h3>\n",
    "\n",
    "\n",
    "This trap occurs when we fail to understand how reusing a model or\n",
    "algorithm that is designed for one specific social context, may not necessarily\n",
    "apply to a different social context. Reusing an algorithmic solution and failing\n",
    "to take into account differences in involved social contexts can result in misleading\n",
    "results and potentially harmful consequences if the algorithm is used to determine the\n",
    "fate of an individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common \"traps\" we can fall into</h2>\n",
    "<h3 align='center'>The Framing Trap</h3>\n",
    "\n",
    "This trap occurs when we fail to consider the full picture surrounding\n",
    "a particular social context when abstracting a social problem, and implementing an\n",
    "algorithm in which the outcome involves enforcing decisions that will impact a person\n",
    "or group of people. It can occur at the data collection, data selection for learning, data processing, and labelling stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>What does this look like in practice?</h2>\n",
    "\n",
    "\n",
    "\n",
    "Reusing a machine learning algorithm used to screen job applications in the\n",
    "nursing industry, for job applications in the information technology sector. An intuitive\n",
    "yet important difference between both contexts is the difference in skills required to\n",
    "succeed in both industries. A slightly more subtle difference is the demographic differences\n",
    "in number of genders who typically work in each of these industries, resulting from wording in job postings,\n",
    "social constructs on gender and societal roles, and the male-female ratio of successful\n",
    "applicants in each field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Introducing FairLearn:<br> a Python library focused on decreasing unfairness in machine learning models </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This dataset is a classification problem - given a range of data about 32,000 individuals, predict whether their annual income is above or below fifty thousand dollars per year.\n",
    "\n",
    "For the purposes of this notebook, we shall treat this as a loan decision problem. The label indicates whether or not each individual repaid a loan in the past. We will use the data to train a predictor to predict whether previously unseen individuals will repay a loan or not. \n",
    "\n",
    "**The assumption is that the model predictions are used to decide whether an individual should be offered a loan.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import DemographicParity, ErrorRate\n",
    "from fairlearn.metrics import MetricFrame, selection_rate#, count\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as skm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "X_raw = data.data\n",
    "Y = (data.target == '>50K') * 1\n",
    "X_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to treat the sex of each individual as a sensitive feature (where 0 indicates female and 1 indicates male), and in this particular case we are going separate this feature out and drop it from the main data. We then perform some standard data preprocessing steps to convert the data into a format suitable for the ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Isolate 'sex' column, create dataframe without 'sex'\n",
    "A = X_raw[\"sex\"]\n",
    "X = X_raw.drop(labels=['sex'], axis=1)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Scale data \n",
    "sc = StandardScaler()\n",
    "# Apply scaling to dataframe\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Set up label encoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(X_scaled,\n",
    "                                                                     Y,\n",
    "                                                                     A,\n",
    "                                                                     test_size=0.2,\n",
    "                                                                     random_state=0,\n",
    "                                                                     stratify=Y)\n",
    "\n",
    "# Work around indexing bug\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Training a fairness-unaware predictor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "unmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\n",
    "\n",
    "unmitigated_predictor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can start to assess the predictor's fairness using the `MetricFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "metric_frame = MetricFrame(metric={\"accuracy\": skm.accuracy_score,\n",
    "                                    \"selection_rate\": selection_rate},\n",
    "                           sensitive_features=A_test,\n",
    "                           y_true=Y_test,\n",
    "                           y_pred=unmitigated_predictor.predict(X_test))\n",
    "print(metric_frame.overall)\n",
    "print(metric_frame.by_group)\n",
    "metric_frame.by_group.plot.bar(\n",
    "        subplots=True, layout=[3, 1], legend=False, figsize=[12, 8],\n",
    "        title='Accuracy and selection rate by group');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking at the disparity in accuracy, we see that males have an error about three times greater than the females. More interesting is the disparity in opportunity - males are offered loans at three times the rate of females.\n",
    "\n",
    "Despite the fact that we removed the feature from the training data, our predictor still discriminates based on sex. This demonstrates that simply ignoring a sensitive feature when fitting a predictor rarely eliminates unfairness. There will generally be enough other features correlated with the removed feature to lead to disparate impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Mitigation with GridSearch</h2>\n",
    "\n",
    "The fairlearn.reductions.GridSearch class implements a simplified version of the exponentiated gradient reduction of Agarwal et al. 2018. The user supplies a standard ML estimator, which is treated as a blackbox. GridSearch works by generating a sequence of relabellings and reweightings, and trains a predictor for each.\n",
    "\n",
    "For this example, we specify demographic parity (on the sensitive feature of sex) as the fairness metric. Demographic parity requires that individuals are offered the opportunity (are approved for a loan in this example) independent of membership in the sensitive class (i.e., females and males should be offered loans at the same rate). We are using this metric for the sake of simplicity; in general, the appropriate fairness metric will not be obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up GridSearch\n",
    "sweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit training data using GridSearch\n",
    "sweep.fit(X_train, Y_train,\n",
    "          sensitive_features=A_train)\n",
    "\n",
    "predictors = sweep.predictors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "errors, disparities = [], []\n",
    "for m in predictors:\n",
    "    def classifier(X): return m.predict(X)\n",
    "\n",
    "    error = ErrorRate()\n",
    "    error.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n",
    "    disparity = DemographicParity()\n",
    "    disparity.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n",
    "\n",
    "    errors.append(error.gamma(classifier)[0])\n",
    "    disparities.append(disparity.gamma(classifier).max())\n",
    "\n",
    "all_results = pd.DataFrame({\"predictor\": predictors, \"error\": errors, \"disparity\": disparities})\n",
    "\n",
    "non_dominated = []\n",
    "for row in all_results.itertuples():\n",
    "    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"] <= row.disparity]\n",
    "    if row.error <= errors_for_lower_or_eq_disparity.min():\n",
    "        non_dominated.append(row.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = {\"unmitigated\": unmitigated_predictor.predict(X_test)}\n",
    "metric_frames = {\"unmitigated\": metric_frame}\n",
    "for i in range(len(non_dominated)):\n",
    "    key = \"dominant_model_{0}\".format(i)\n",
    "    predictions[key] = non_dominated[i].predict(X_test)\n",
    "\n",
    "    metric_frames[key] = MetricFrame(metric={\"accuracy\": skm.accuracy_score,\n",
    "                                              \"selection_rate\": selection_rate},\n",
    "                                     sensitive_features=A_test,\n",
    "                                     y_true=Y_test,\n",
    "                                     y_pred=predictions[key])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = [metric_frame.overall['accuracy'] for metric_frame in metric_frames.values()]\n",
    "y = [metric_frame.difference()['selection_rate'] for metric_frame in metric_frames.values()]\n",
    "keys = list(metric_frames.keys())\n",
    "plt.scatter(x, y)\n",
    "for i in range(len(x)):\n",
    "    plt.annotate(keys[i], (x[i] + 0.0003, y[i]))\n",
    "plt.xlabel(\"accuracy\")\n",
    "plt.ylabel(\"selection rate difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>References</h2>\n",
    "\n",
    "[1] Saitta, Lorenza, Zucker, Jean-Daniel, \"Abstraction in Artificial Intelligence and Complex Systems\" (2013), Chapter \"Abstraction in Machine Learning\", pp. 273--327, Springer New York,\"https://doi.org/10.1007/978-1-4614-7052-6_9 \n",
    "\n",
    "[2] Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle and Venkatasubramanian,\n",
    "      Suresh and Vertesi, Janet, \"Fairness and Abstraction in Sociotechnical Systems\" (August 23, 2018).\n",
    "      2019 ACM Conference on Fairness, Accountability, and Transparency (FAT*), 59-68, Available at\n",
    "      `SSRN: \t<https://ssrn.com/abstract=3265913>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
