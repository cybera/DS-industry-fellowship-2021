{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c41bc",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# -q option supresses levels of output\n",
    "!pip install -r requirements.txt -q -q -q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdec47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align='center'>Ethics in Artificial Intelligence</h1>\n",
    "\n",
    "<h3 align='center'>Zachary Shand</h3>\n",
    "\n",
    "<h3 align='center'>with material from Laura G. Funderburk</h3>\n",
    "\n",
    "<h3 align='center'>Cybera</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d250f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!-- <h2 align='center'>About this workshop</h2> -->\n",
    "\n",
    "\n",
    "<!-- In recent years we have seen a rise in the use of artificial intelligence in our everyday lives: from machine learning algorithms that screen resumes for job postings, to bots that can converse with one another. In this workshop, participants will explore the pitfalls of artificial intelligence when it comes to algorithms responsible for making decisions impacting lives, and what we can do to mitigate unfairness of our models.  -->\n",
    "\n",
    "<center>AI and ML affect our everyday lives: from jobs, to twitter bots, to the justice system.\n",
    "\n",
    "<center>But are these systems fair?\n",
    "    \n",
    "<center>What is <em>fair</em>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37a421",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Workshop at a glance</h2>\n",
    "\n",
    "1. Theory\n",
    "    1. Motivating the importance of ethics in artificial intelligence (AI).\n",
    "    2. How to determine source of unfairness in AI.\n",
    "        1. Measurements and Fairness\n",
    "        2. Traps in Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e4d95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!-- <h2 align='center'>Workshop at a glance</h2> -->\n",
    "\n",
    "\n",
    "2. Practice\n",
    "    1. Introducte the Fairlearn Python library\n",
    "    2. Create a basic classifier with medical data\n",
    "    3. Evaluate 'fairness' in model\n",
    "    4. Mitigate bias with Fairlearn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daecd52a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Motivating Ethical Artificial Intelligence</h2>\n",
    "\n",
    "<!-- Consider risk assessment of relapse in criminal behaviour and recommendations for appropriate measures to prevent relapse.\n",
    "\n",
    "**Your task:**  develop a model that takes as input information about the case, and as output it returns a score that will be used to determine an appropriate course of action.  -->\n",
    "\n",
    "COMPAS: risk of recidivism (US)\n",
    "* model has demographic parity,\n",
    "* sees racial bias reinforcement in usage\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/handcuff.jpeg\" width=\"500\"/>|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2243107",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Motivating Ethical Artificial Intelligence</h2>\n",
    "\n",
    "Teacher Value Add Model: \n",
    "* produces unreliable output, \n",
    "* evaluated on standard test scores, \n",
    "* used for funding (reinforces low income areas low scores),\n",
    "* metric for hiring/firing\n",
    "\n",
    "Patient Risk: \n",
    "* enrolls patients in program based on costs, \n",
    "* racially biased, 17% of enrolled patients are black (vs. white in binary analysis)\n",
    "* care needs are operationalize on past cost which blends care _need_ and _access_ ([Obermeyer et al., 2019](https://science.sciencemag.org/content/366/6464/447.full))\n",
    "\n",
    "Universal Credits benefit (UK): system misestimated appliants' income and witholds economic benefits. \n",
    "\n",
    "Amazon Recruitment: hiring AI project scrapped after it learns bias against women. [Reuters 2018](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5e0d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Why Ethics in AI matter</h2>\n",
    "\n",
    "\n",
    "AI systems can behave unfairly for a variety of reasons: \n",
    "\n",
    "1. Societal biases are reflected in the training data.  \n",
    "2. Societal biases are reflected in the decisions made during the development and deployment of these systems. \n",
    "\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/bias.jpeg\" width=\"400\"/>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2dcf2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Types of harms</h2>\n",
    "\n",
    "**Allocation harms** can occur when AI systems extend or withhold\n",
    "  opportunities, resources, or information. \n",
    "\n",
    "*Sample key applications: hiring, school admissions, and lending.*\n",
    "\n",
    "**Quality-of-service harms** can occur when a system does not work as well for\n",
    "  one person as it does for another, even if no opportunities, resources, or\n",
    "  information are extended or withheld.\n",
    "\n",
    "*Sample key applications: accuracy in face recognition, document search, or product recommendation.*\n",
    "  \n",
    "See also keynote by K. Crawford at NeurIPS 2017 <https://www.youtube.com/watch?v=fMym_BKWQzk>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c11b21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Measurment & Fairness</h2>\n",
    "<!-- <h2 align='center'>How do we fall into misrepresentations in machine learning?</h2> -->\n",
    "\n",
    "Computers model _unobservable theoretical constructs_ by _operatinalizing_ them via a _measurement model_.\n",
    "\n",
    "**What is fairness?**\n",
    " * Who are are protected groups?\n",
    "     * Race, sex, age, **# what else #**\n",
    " * What metric or number defines fairness?\n",
    " * Can we have group and individual fairness?\n",
    "\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/noface.png\" width=\"400\"/>|\n",
    "\n",
    "<!-- How do we determine which properties are *worth* preserving and describing? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056a415",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Measurement & Fairness in Quantitative Social Sciences</h2>\n",
    "\n",
    "\n",
    " * Construct Reliability \n",
    "     * Do similar inputs have similar outputs?\n",
    "     * Is there test-retest reliability?\n",
    "     * Can we even measure when inputs are similar?\n",
    " * Construct Validity\n",
    "     * Is the data unbiased?\n",
    "     * Is the model content suitable?\n",
    "\n",
    "See [Jacobs & Wallach 2021](https://dl.acm.org/doi/abs/10.1145/3442188.3445901) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060968a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align='center'> Construct Validity: </h3>\n",
    "\n",
    "* Face Validity\n",
    "    * Does the data/output make sense?\n",
    "* Content Validity: \n",
    "    * Is our construct contested, substantive, and structurally valid?\n",
    "* Convergent Validity:\n",
    "    * Are our measurements and outputs consistent with other valid constructs?\n",
    "* Discriminant Validity:\n",
    "    * Does the measurements correlate in expected ways with our construct?\n",
    "    * Are we accidentally capturing/mixing other measurements?\n",
    "* Predictive Validity:\n",
    "    * Do the predictions match other (non-included) properties/constructs?\n",
    "* Hypothesis Validity: \n",
    "    * Does this support a \"substantially interesting hypothesis?\"\n",
    "* Consequential Validity:\n",
    "    * Will using this construct/model provide a positive impact?\n",
    "    \n",
    "See [Jacobs & Wallach 2021](https://dl.acm.org/doi/abs/10.1145/3442188.3445901) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31422f9f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The algorithmic frame</h3>\n",
    "\n",
    "Frame centered around choices made when abstracting a problem in the form of representations (input data) and labelling (outcome). \n",
    "\n",
    "Evaluated on accuracy and generalizability to data the model did not train on. \n",
    "\n",
    "**Fairness cannot be defined in this frame $\\Rightarrow$ goal is to produce a model that best captures the relationship between representations and labels.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79125ca3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The data frame</h3>\n",
    "\n",
    "This frame is concerned with the quality of representations (input data) and the outcomes that result. \n",
    "\n",
    "**This additional frame allows us to question the inherent (un)fairness present in input and output data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee6585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The sociotechnical frame</h3>\n",
    "\n",
    "$$\\text{Technical Component} + \\text{Social Component} = \\text{Sociotechnical system}$$\n",
    "\n",
    "\n",
    "Recognizes that a machine learning model is part of the interaction between people and technology. \n",
    "\n",
    "Any social components of this interaction need to be modelled and incorporated accordingly.\n",
    "\n",
    "**Designers of machine learning systems who fail to consider the way in which social contexts and technologies\n",
    "are interrelated are at risk of falling into \"abstraction traps\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777813d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>What traps can we fall into when modeling a social problem?</h2>\n",
    "\n",
    "\n",
    "Machine learning systems used in the real world are inherently sociotechnical\n",
    "systems.\n",
    "\n",
    "Designers of machine learning systems typically translate a real-world context into a machine learning\n",
    "model through abstraction: \n",
    "\n",
    "* Focus on 'relevant' aspects of that context (input, output, relationship)\n",
    "\n",
    "* Problems arise when we abstract away the social context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403d09c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "\n",
    "[Selbst et al. 2018](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3265913) identify five traps we can fall into when implementing a machine learning model:\n",
    "\n",
    "* The Framing Trap\n",
    "\n",
    "* The Portability Trap\n",
    "\n",
    "* The Formalism Trap\n",
    "\n",
    "* The Ripple Effect Trap\n",
    "\n",
    "* The Solutionism Trap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288d477",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Framing Trap</h3>\n",
    "\n",
    "Occurs when failing to consider entire system and context: \n",
    "\n",
    "* Stuck in the algorithmic frame (accuracy, ROC, etc.)\n",
    "\n",
    "* Consider the data frame (data bias, balance)\n",
    "\n",
    "* Expanded sociotechnical frame includes deployment, use, consequences of outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf188a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Portability Trap</h3>\n",
    "\n",
    "How will a system generalize to other domains?\n",
    "* Hiring in different sectors.\n",
    "* Will our fairness guarantees transfer to new data?\n",
    "* How will a ported system fit into a sociotechnical context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1018a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Formalism Trap</h3>\n",
    "\n",
    "Fairness is a complex construct that is contested:\n",
    "* Demographic parity vs. equalized odds vs. equalized opportunity vs. selection rate vs. etc.\n",
    "\n",
    "Group fairness metrics do not account for differences in individual experiences nor do they account for procedural justice.\n",
    "\n",
    "Consider: procedurality (law), contextuality (wrongful discriminatino) and contestability (shifting definitions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aff608",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Ripple Effect Trap</h3>\n",
    "\n",
    "Consider: \n",
    "* Will people game the system?\n",
    "* Does it reinforce unfair/biased decisions?\n",
    "* Can it be used maliciously, or is it robust to political or policy changes?\n",
    "* Does it priveledge certain constructs over others (e.g. recivism risk over prevention or rehabilitation)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96aed9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Solutionism Trap</h3>\n",
    "\n",
    "\"If you have a a hammer everything looks like a nail.\"\n",
    "\n",
    "Can we reasonably construct a technical solution for the problem? \n",
    "\n",
    "**Solutionist approaches also fail as definitions of fairness are contested over time**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2edc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Introducing FairLearn:<br> a Python library focused on decreasing unfairness in machine learning models </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ad254",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>About Fairlearn</h2>\n",
    "\n",
    "Fairlearn is an open-source, community-driven project to help data scientists improve fairness of AI systems. It includes:\n",
    "\n",
    "* A Python library for fairness assessment and improvement (fairness metrics, mitigation algorithms, plotting, etc.)\n",
    "\n",
    "* Educational resources covering organizational and technical processes for unfairness mitigation (user guide, case studies, Jupyter notebooks, etc.)\n",
    "\n",
    "The project was started in 2018 at Microsoft Research. In 2021 it adopted neutral governance structure and since then it is completely community-driven.\n",
    "\n",
    "Read more: https://fairlearn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777efc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Hands on code section</h2>\n",
    "\n",
    "The key steps in our processing will be (assuming we start with a clean dataset):\n",
    "\n",
    "1. Load and examine dataset.\n",
    "\n",
    "2. Train and evaluate logistic regression model.\n",
    "\n",
    "3. Examine protected class distribution and establish fairness metric. \n",
    "\n",
    "4. Examine Construct Validity.\n",
    "\n",
    "5. Evaluate fairness and postprocess model using Fairlearn.\n",
    "\n",
    "6. Did we fall into any of the traps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131302a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Example with Health Data</h2>\n",
    "\n",
    "In this tutorial, we will work with a [publicly available clinical dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008) that focuses on diabetic patients ([Strack et al., 2014](https://www.hindawi.com/journals/bmri/2014/781670/)).\n",
    "\n",
    "    Clincial dataset of hospital re-admissions over a ten-year period (1998-2008) for diabetic patients across 130 different hospitals in the US.\n",
    "\n",
    "    The features include: demographics, diagnoses, diabetic medications, number of visits in the year preceding the encounter, and payer information, whether the patient was readmitted after release, and whether the readmission occurred within 30 days of the release.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71b7b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align='center'>Decision point: Task definition</h3>\n",
    "\n",
    "* A hospital **readmission within 30 days** can be viewed as a proxy that the patients needed more assistance at the release time, so it will be the label we wish to predict.\n",
    "\n",
    "* Because of the class imbalance, we will be measuring our performance via **balanced accuracy**. Another key performance consideration is how many patients are recommended for care, metric we refer to as **selection rate**.\n",
    "\n",
    "Ideally, health care professionals would be involved in both designing and using the model, including formalizing the task definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61118ab7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%run -i process_health_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab016e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbfc31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,sharey=True)\n",
    "fig.set_size_inches(12,6)\n",
    "sns.barplot(data=df, y='readmit_30_days', ax=axs[0]); axs[0].set_xlabel(\"Total Population\")\n",
    "sns.barplot(data=df, y='readmit_30_days', x='A1Cresult', ax=axs[1])\n",
    "sns.histplot(data=df, x='time_in_hospital', ax= axs[2], bins=[1, 5, 10, 14], hue='readmit_30_days', stat='density');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9c44f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prepare training and test datasets\n",
    "\n",
    "As we mentioned in the task definition, our target variable is **readmission within 30 days**, and our sensitive feature for the purposes of fairness assessment is **race**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77377f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    X_train, X_test, \n",
    "    Y_train, Y_test, \n",
    "    A_train, A_test, \n",
    "    df_train, df_test, \n",
    "    X\n",
    ") = train_test_class_split(df, test_size=0.2, drop_race=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116ebd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We train a logistic regression model and save its predictions on test data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913745d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "unmitigated_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"logistic_regression\", \n",
    "     LogisticRegression(penalty='none', fit_intercept=True, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit data\n",
    "unmitigated_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "#predict on test set\n",
    "Y_pred_proba = unmitigated_pipeline.predict_proba(X_test)[:,1]\n",
    "Y_pred = unmitigated_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69256c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate model perfomance\n",
    "\n",
    "Check standard performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a34f88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ax = sklearn.metrics.RocCurveDisplay.from_estimator(unmitigated_pipeline, X_test, Y_test)\n",
    "plt.gca().plot([0,1], [0,1], ls='dashed', color='k', label='Random Guess')\n",
    "plt.gca().legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc6215",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Show balanced accuracy rate of the 0/1 predictions\n",
    "balanced_accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0524c7f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we see, the performance of the model is well above the performance of a coin flip (whose performance would be 0.5 in both cases), albeit it is quite far from a perfect classifier (whose performance would be 1.0 in both cases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae3b6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examine Protected Classes\n",
    "\n",
    "Small sample sizes have two implications:\n",
    "\n",
    "* **assessment**: the impacts of the AI system on smaller groups are harder to assess, because due to fewer data points we have a much larger uncertainty (error bars) in our estimates\n",
    "\n",
    "* **model training**: fewer training data points mean that our model fails to appropriately capture any data patterns specific to smaller groups, which means that its predictive performance on these groups could be worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0ab20",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "protected_vars = ['race'] # what others?\n",
    "fig, axs = plt.subplots(1,len(protected_vars),squeeze=False)\n",
    "axs = axs[0]\n",
    "fig.set_size_inches(6*len(protected_vars), 6)  \n",
    "\n",
    "for label, ax in zip(protected_vars, axs):\n",
    "    sns.countplot(data=df[protected_vars], x=label, ax=ax)\n",
    "    ax.tick_params(axis='x', rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a35e05",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In our dataset, our patients are predominantly *Caucasian* (75%). The next largest racial group is *AfricanAmerican*, making up 19% of the patients. The remaining race categories (including *Unknown*) compose only 6% of the data.\n",
    "\n",
    "Gender is in our case effectively binary (and we have no further information how it was operationalized), with both *Female* represented at 54% and *Male* represented at 46%. There are only 3 samples annotated as *Unknown/Invalid*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ac5aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Label imbalance\n",
    "\n",
    "* some classification algorithms and performance measures might not work well with data sets with extreme class imbalance\n",
    "* in binary classification settings, our ability to evaluate error is often driven by the size of the smaller of the two classes (again, the smaller the sample the larger the uncertainty in estimates)\n",
    "* label imbalance may exacerbate the problems due to smaller group sizes in fairness assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cae3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1)\n",
    "ax = sns.barplot(data=df, x='gender', y='readmit_30_days', hue='race', ax=axs[0])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "ax = sns.barplot(data=df, x='race', y='readmit_30_days', hue='gender', ax=axs[1])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "fig.set_size_inches(16,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77eeee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content Validity\n",
    "\n",
    "We check the coefficients of the fitted model to make sure that they \"makes sense\". While subjective, this step is important and helps catch mistakes and might point out to some fairness issues. However, we will systematically assess the fairness of the model in the next section.\n",
    "\n",
    "Coefficients represent the model's log-odds. Doubling a categorical variable, or increasing a continuous variable by a standard deviation, doubles the odds of the classifier going towards the “1” case. This is a true statistical measure (according to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5b854",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "fig, axs = plt.subplots(1,2,sharex=True)\n",
    "coef_series = pd.Series(data=unmitigated_pipeline.named_steps[\"logistic_regression\"].coef_[0], index=X.columns)\n",
    "N = len(coef_series)\n",
    "coef_series.sort_values().iloc[N//2:].plot.barh(figsize=(4, 12), legend=False, ax=axs[0]);\n",
    "coef_series.sort_values().iloc[:N//2].plot.barh(figsize=(4, 12), legend=False, ax=axs[1]);\n",
    "axs[1].yaxis.set_label_position(\"right\")\n",
    "axs[1].yaxis.tick_right()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fee8f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discriminant Validity & Predictive Validity\n",
    "\n",
    "We would like to show that our measurement `readmit_30_days` is correlated with patient characteristics that are related to our construct \"benefiting from care management\". One such characteristic is the general patient health, where we expect that patients that are less healthy are more likely to benefit from care management.\n",
    "\n",
    "While our data does not contain full health records that would enable us to holistically measure general patient health, the data does contain two relevant features: `had_emergency` and `had_inpatient_days`, which indicate whether the patient spent any days in the emergency room or in the hospital (but non-emergency) in the preceding year.\n",
    "\n",
    "To establish predictive validity, we would like to show that our measurement `readmit_30_days` is predictive of these two observable characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a4f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pointplot(df, \"race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca728fb6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "In all cases, we see that readmission in 30 days is predictive of our two measurements of general patient health.\n",
    "\n",
    "The analysis is also surfacing the fact that patients with the value of race *Unknown* have fewer hospital visits in the preceding year (both emergency and non-emergency) than other groups. In practice, this would be a good reason to reach out to health professionals to investigate this patient cohort, to make sure that we understand why there is the systematic difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b49fcc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We see the rate of *30-day readmission* is similar for the *AfricanAmerican* and *Caucasian* groups, but appears smaller for *Other* and smallest for *Unknown* (this is consistent with an overall lower rate of hospital visits in the prior year). The smaller sample size of the *Other* and *Unknown* groups mean that there is more uncertainty around the estimate for these two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95267e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergent Validity\n",
    "Other research indicates that `AfricanAmerican` and `Caucasian` labels have fairness/bias issues in US healthcare.\n",
    "\n",
    "## Consequential Validity\n",
    "Does using such a low accuracy classifier positively impact a healthcare setting?\n",
    "\n",
    "## Hypothesis Validity\n",
    "What could be some \"substantially interesting hypotheses\" from this model?\n",
    "\n",
    "## Face Validity\n",
    "Does this all make sense so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6df76a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fairness assessment with `MetricFrame`\n",
    "\n",
    "Fairlearn provides the data structure called `MetricFrame` to enable evaluation of disaggregated metrics. We will show how to use a `MetricFrame` object to assess the trained `LogisticRegression` classifier for potential fairness-related harms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f8ffc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Fairness considerations**\n",
    "\n",
    "* _Which groups are most likely to be disproportionately negatively affected?_ Previous work suggests that groups with different race and ethnicity can be differently affected.\n",
    "\n",
    "* _What are the harms?_ The key harms here are allocation harms. In particular, false negatives, i.e., don't recommend somebody who will be readmitted.\n",
    "\n",
    "* _How should we measure those harms?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3961acf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In its simplest form MetricFrame takes four arguments:\n",
    "#    metric_function with signature metric_function(y_true, y_pred)\n",
    "#    y_true: array of labels\n",
    "#    y_pred: array of predictions\n",
    "#    sensitive_features: array of sensitive feature values\n",
    "\n",
    "# You can also evaluate multiple metrics by providing a dictionary\n",
    "\n",
    "metrics_dict = {\n",
    "    \"selection_rate\": selection_rate,\n",
    "    \"false_negative_rate\": false_negative_rate,\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,\n",
    "}\n",
    "\n",
    "metricframe_unmitigated = MetricFrame(metrics=metrics_dict,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])\n",
    "\n",
    "# The disaggregated metrics are then stored in a pandas DataFrame:\n",
    "\n",
    "metricframe_unmitigated.by_group.sort_values(by='false_negative_rate')[['false_negative_rate']] ## also look at all multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0262fd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio, and the maximum and minimum values\n",
    "# across the groups are then all pandas Series, for example:\n",
    "\n",
    "metricframe_unmitigated.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a716ecc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio, and the maximum and minimum values\n",
    "# across the groups are then all pandas Series, for example:\n",
    "\n",
    "# You'll probably want to view them transposed:\n",
    "pd.DataFrame({'difference': metricframe_unmitigated.difference(),\n",
    "              'ratio': metricframe_unmitigated.ratio(),\n",
    "              'group_min': metricframe_unmitigated.group_min(),\n",
    "              'group_max': metricframe_unmitigated.group_max()}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d4adc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# You can also easily plot all of the metrics using DataFrame plotting capabilities\n",
    "\n",
    "metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
    "                      legend=False, rot=-45, position=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb001e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "According to the above bar chart, it seems that the group *Unknown* is selected for the care management program less often than other groups as reflected by the selection rate. Also this group experiences the largest false negative rate, so a larger fraction of group members that are likely to benefit from the care management program are not selected. Finally, the balanced accuracy on this group is also the lowest.\n",
    "\n",
    "\n",
    "We observe disparity, even though we did not include race in our model. There's a variety of reasons why such disparities may occur. It could be due to representational issues (i.e., not enough instances per group), or because the feature distribution itself differs across groups (i.e., different relationship between features and target variable, obvious example would be people with darker skin in facial recognition systems, but can be much more subtle). Real-world applications often exhibit both kinds of issues at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908301e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Postprocessing with `ThresholdOptimizer`\n",
    "\n",
    "**Postprocessing** techniques are a class of unfairness-mitigation algorithms that take an already trained model and a dataset as an input and seek to fit a transformation function to model's outputs to satisfy some (group) fairness constraint(s). They might be the only feasible unfairness mitigation approach when developers cannot influence training of the model, due to practical reasons or due to security or privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf8341",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we use the `ThresholdOptimizer` algorithm from Fairlearn, which follows the approach of [Hardt, Price, and Srebro (2016)](https://arxiv.org/abs/1610.02413).\n",
    "\n",
    "`ThresholdOptimizer` takes in an exisiting (possibly pre-fit) machine learning model whose predictions act as a scoring function and identifies a separate thrceshold for each group in order to optimize some specified objective metric (such as **balanced accuracy**) subject to specified fairness constraints (such as **false negative rate parity**). Thus, the resulting classifier is just a suitably thresholded version of the underlying machinelearning model.\n",
    "\n",
    "The constraint **false negative rate parity** requires that all the groups have equal values of false negative rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e80817",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To instatiate our `ThresholdOptimizer`, we pass in:\n",
    "\n",
    "*   An existing `estimator` that we wish to threshold. \n",
    "*   The fairness `constraints` we want to satisfy.\n",
    "*   The `objective` metric we want to maximize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90458f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we instantite ThresholdOptimizer with the logistic regression estimator\n",
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=unmitigated_pipeline,\n",
    "    constraints=\"false_negative_rate_parity\", #is this more fair than equal_opportunity, demographic_parity?\n",
    "    objective=\"balanced_accuracy_score\",\n",
    "    prefit=True,\n",
    "    predict_method='predict_proba'\n",
    ")\n",
    "\n",
    "postprocess_est.fit(X_train, Y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819516b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to use the `ThresholdOptimizer`, we need access to the sensitive features **both during training time and once it's deployed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b00e82",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Record and evaluate the output of the trained ThresholdOptimizer on test data\n",
    "\n",
    "Y_pred_postprocess = postprocess_est.predict(X_train, sensitive_features=A_train)\n",
    "metricframe_postprocess = MetricFrame(\n",
    "    metrics=metrics_dict,\n",
    "    y_true=Y_train,\n",
    "    y_pred=Y_pred_postprocess,\n",
    "    sensitive_features=A_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657902f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.by_group,\n",
    "           metricframe_postprocess.by_group],\n",
    "           keys=['Unmitigated', 'ThresholdOptimizer'],\n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dddbe3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.difference(),\n",
    "           metricframe_postprocess.difference()],\n",
    "          keys=['Unmitigated: difference', 'ThresholdOptimizer: difference'],\n",
    "          axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18d8a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "metricframe_postprocess.by_group.plot.bar(subplots=True, layout=[1,3], figsize=(12, 4), legend=False, rot=-45, position=1.5)\n",
    "postprocess_performance = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3d4d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fairlearn's Post-Processing Optimization \n",
    "\n",
    "Depending on the fairness constraint, the optimization done is slightly different. \n",
    "For `false_negative_rate` parity: \n",
    "    * Find best option for worst classfied group and use as threshold.\n",
    "\n",
    "Other metrics (e.g. equalized odds) perform different optizations (e.g. selecting points on ROC curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440c782",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import fairlearn\n",
    "fairlearn.postprocessing.plot_threshold_optimizer(postprocess_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120ecc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compare Mitigated and Unmitigated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3beed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# You can also easily plot all of the metrics using DataFrame plotting capabilities\n",
    "\n",
    "axs = metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
    "                      legend=False, rot=-45, position=1.5);\n",
    "for ax in axs.flatten():\n",
    "    ax.set_ylim(0, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e391e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "axs = metricframe_postprocess.by_group.plot.bar(subplots=True, layout=[1,3], figsize=(12, 4), legend=False, rot=-45, position=1.5)\n",
    "for ax in axs.flatten():\n",
    "    ax.set_ylim(0, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df674878",
   "metadata": {},
   "source": [
    "## Review Traps\n",
    "Did we fall into of these traps?\n",
    "* The Framing Trap\n",
    "\n",
    "* The Portability Trap\n",
    "\n",
    "* The Formalism Trap\n",
    "\n",
    "* The Ripple Effect Trap\n",
    "\n",
    "* The Solutionism Trap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2e32b",
   "metadata": {},
   "source": [
    "# Possible Excercises\n",
    "\n",
    "* How do the results above change with smaller test/train splits?\n",
    "* Does including race increase, decrease or have no effect on the bias?\n",
    "* Do all variables satistfy discriminant validity and/or content validity (substantive validity)?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab63ec2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Final remarks</h2>\n",
    "\n",
    "In this workshop we motivated the importance of incorporating and assessing fairness metrics as part of a problem that uses machine learning techniques. \n",
    "\n",
    "We covered an overview of what abstraction is, how harms in abstraction take place, and the different traps we can fall into when our abstraction does not incorporate social context. \n",
    "\n",
    "We applied what we learned to a scenario involving patient enrollment in high-risk care management program. We explored the Fairlearn Python library to improve unfair outcomes towards patients whose race was unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fbe53",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Final remarks</h2>\n",
    "\n",
    "In our labelled data set, it was easy to calculate metrics for racial bias. \n",
    "\n",
    "Other domains and types of machine learning may provide other challenges. \n",
    "\n",
    "Example, [clustering legal documents](https://www.prnewswire.com/news-releases/everlaw-launches-ai-based-clustering-to-open-a-new-world-of-ediscovery-insights-to-legal-teams-301563544.html?utm_campaign=The%20Batch&utm_medium=email&_hsmi=218814666&_hsenc=p2ANqtz-9-anU6zKEu8IzHnPTUeiZqmgmfTtNVrVH8XDisoEdavGqOvyWnrOHcouxF4oF5ikMzkWkEEw2EnM6AxaMNx4tpCKTTvQ&utm_content=218804890&utm_source=hs_email) for discovery during legal.\n",
    "* Is there a fairness problem?\n",
    "* How would you measure this in an unsupervised clustering situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa14c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Future readings and exercises:\n",
    "\n",
    "You can learn more about Fairlearn https://fairlearn.org/v0.7.0/about/index.html and visit their open source code https://github.com/fairlearn/fairlearn\n",
    "\n",
    "Fairlearn is a community-based effort to improve ethics in AI. \n",
    "\n",
    "Curious to contribute? Visit https://fairlearn.org/v0.7.0/contributor_guide/index.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a4e85b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Workshop's sources of inspiration</h2>\n",
    "\n",
    "**Literature:**\n",
    "\n",
    "Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle and Venkatasubramanian, Suresh and Vertesi, Janet, \"Fairness and Abstraction in Sociotechnical Systems\", (August 23, 2018). 2019 ACM Conference on Fairness, Accountability, and Transparency (FAT*), 59-68, Available at SSRN: \t<https://ssrn.com/abstract=3265913>,\n",
    "\n",
    "Jacobs, Abigail Z., and Hanna Wallach. \"Measurement and fairness.\" Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2021\n",
    "\n",
    "**Fairlearn's 2021 SciPy tutorial:**\n",
    "\n",
    "SciPy 2021 Tutorial: Fairness in AI systems: From social context to practice using Fairlearn by Manojit Nandi, Miroslav Dudík, Triveni Gandhi, Lisa Ibañez, Adrin Jalali, Michael Madaio, Hanna Wallach, Hilde Weerts is licensed under\n",
    "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "**Protected Classes and Discrimination in Canada Legislature:**\n",
    "\n",
    "[Government of Canada: Rights in the Workplace](https://www.canada.ca/en/canadian-heritage/services/rights-workplace.html)\n",
    "\n",
    "[Canadian Human Rights Act 1985](https://laws-lois.justice.gc.ca/eng/acts/h-6/)\n",
    "\n",
    "[Canadian human rights commision | Commision canadienne des droits de la personned](https://www.chrc-ccdp.gc.ca/en/about-human-rights/what-discrimination)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4540c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/development.jpeg\" width=\"500\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e2111",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/tradeoff.jpeg\" width=\"500\"/>|"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "rise": {
   "autolaunch": false,
   "backimage": "Cybera_contact.svg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
