{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align='center'>Ethics in Artificial Intelligence</h1>\n",
    "\n",
    "<h3 align='center'>Laura G. Funderburk</h3>\n",
    "\n",
    "<h3 align='center'>Data Scientist, Cybera</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>About this workshop</h2>\n",
    "\n",
    "\n",
    "In recent years we have seen a rise in the use of artificial intelligence in our everyday lives: from machine learning algorithms that screen resumes for job postings, to bots that can converse with one another. In this workshop, participants will explore the pitfalls of artificial intelligence when it comes to algorithms responsible for making decisions impacting lives, and what we can do to mitigate unfairness of our models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Workshop at a glance</h2>\n",
    "\n",
    "* Theory\n",
    "\n",
    "    1. Motivating the importance of ethics in artificial intelligence (AI).\n",
    "    2. How to determine source of unfairness in AI.\n",
    "    3. The role of abstraction in AI.\n",
    "    4. Frameworks in machine learning abstraction.\n",
    "    5. Traps we can fall into when abstracting.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Workshop at a glance</h2>\n",
    "\n",
    "\n",
    "* Practice\n",
    "\n",
    "    1. Introduction to the Fairlearn Python library.\n",
    "    2. Problem in health care: developing a classification model (patient enrollment in high-risk care management program).\n",
    "    3. Identifying fairness metrics.\n",
    "    4. Summary stats, splitting and training data.\n",
    "    5. Model accuracy and fairness assessment. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Motivating Aritificial Intelligence</h2>\n",
    "\n",
    "Consider risk assessment of relapse in criminal behaviour and recommendations for appropriate measures to prevent relapse.\n",
    "\n",
    "**Your task:**  develop a model that takes as input information about the case, and as output it returns a score that will be used to determine an appropriate course of action. \n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/handcuff.jpeg\" width=\"500\"/>|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Motivating Aritificial Intelligence</h2>\n",
    "\n",
    "\n",
    "**Question 1: How would you approach this problem?**\n",
    "\n",
    "**Question 2: Would a model that incorporates fairness matter?**\n",
    "\n",
    "**Question 3: What does it mean for our model to be unfair?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Workshop's sources of inspiration</h2>\n",
    "\n",
    "Literature:\n",
    "\n",
    "Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle and Venkatasubramanian, Suresh and Vertesi, Janet, \"Fairness and Abstraction in Sociotechnical Systems\", (August 23, 2018). 2019 ACM Conference on Fairness, Accountability, and Transparency (FAT*), 59-68, Available at SSRN: \t<https://ssrn.com/abstract=3265913>,\n",
    "\n",
    "Fairlearn's 2021 SciPy tutorial: \n",
    "\n",
    "SciPy 2021 Tutorial: Fairness in AI systems: From social context to practice using Fairlearn by Manojit Nandi, Miroslav Dudík, Triveni Gandhi, Lisa Ibañez, Adrin Jalali, Michael Madaio, Hanna Wallach, Hilde Weerts is licensed under\n",
    "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Why Ethics in AI matter</h2>\n",
    "\n",
    "\n",
    " AI systems can behave unfairly for a variety of reasons: \n",
    "\n",
    "1. Societal biases are reflected in the training data.  \n",
    "\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/bias.jpeg\" width=\"400\"/>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Why Ethics in AI matter</h2>\n",
    "\n",
    " AI systems can behave unfairly for a variety of reasons: \n",
    "\n",
    "2. Societal biases are reflected in the decisions made during the development and deployment of these systems. \n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/development.jpeg\" width=\"500\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Why Ethics in AI matter</h2>\n",
    "\n",
    " AI systems can behave unfairly for a variety of reasons: \n",
    "\n",
    "3. AI systems behave unfairly because of characteristics of the data or characteristics of the systems themselves.\n",
    "\n",
    "$$\\Rightarrow \\text{They are not mutually exclusive and often exacerbate one another} \\Leftarrow$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Motivating example</h2>\n",
    "\n",
    "Consider the use of \"risk scores\" to decide the outcomes individuals\n",
    "will experience. \n",
    "\n",
    "- Risk of recidivism in crime.\n",
    "- Risk of failure within a new role during recruitment.\n",
    "- Risk of failure in a patient's chosen treatment.\n",
    "\n",
    "$\\Rightarrow$ Does the score accurately depict reality? \n",
    "\n",
    "$\\Rightarrow$  What is the impact of a wrongful assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>How can we determine whether an AI is behaving unfairly?</h2>\n",
    "\n",
    "* Identify underlying misconceptions within AI:\n",
    "    1. Causal: societal-based contenxt/bias\n",
    "    2. Intent-bases: prejudice\n",
    "    \n",
    "* Through study of impact of AI on people:\n",
    "    1. Harms \n",
    "    2. Gains\n",
    "\n",
    "**For this workshop, we define whether an AI system is behaving unfairly in terms of its impact on people.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>A note on the word bias</h2>\n",
    "\n",
    "For this presentation I will refer to fairness in terms of **harms** rather than specific **causes** (such as societal-based context), I avoid the usage of the words *bias* and focus more on the relevant social context that might lead to a harm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Types of harms</h2>\n",
    "\n",
    "From keynote by K. Crawford at NeurIPS 2017 <https://www.youtube.com/watch?v=fMym_BKWQzk>\n",
    "\n",
    "* *Allocation harms* can occur when AI systems extend or withhold\n",
    "  opportunities, resources, or information. \n",
    "  \n",
    "  **Sample key applications: hiring, school admissions, and lending.**\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>Types of harms</h2>\n",
    "\n",
    "From keynote by K. Crawford at NeurIPS 2017 <https://www.youtube.com/watch?v=fMym_BKWQzk>\n",
    "\n",
    "\n",
    "* *Quality-of-service harms* can occur when a system does not work as well for\n",
    "  one person as it does for another, even if no opportunities, resources, or\n",
    "  information are extended or withheld. \n",
    "  \n",
    "  **Sample key applications: accuracy in face recognition, document search, or product recommendation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>How do we incur into a harm?</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we work with information: abstraction</h2>\n",
    "\n",
    "\n",
    "**Abstracting in computer science**  \n",
    "\n",
    "**Abstracting in mathematics** \n",
    "\n",
    "**Abstracting in machine learning** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How do we fall into misrepresentations in machine learning?</h2>\n",
    "\n",
    "\n",
    "$\\Rightarrow$ Abstraction leads to removal of attributes or properties that have dependence on a social context.\n",
    "\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/noface.png\" width=\"400\"/>|\n",
    "\n",
    "How do we determine which properties are *worth* preserving and describing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How do we fall into misrepresentations in machine learning?</h2>\n",
    "\n",
    "$\\Rightarrow$ What is the trade off we make when discarding a property, and are we aware of the consequences of that trade off?\n",
    "\n",
    "| |\n",
    "|-|\n",
    "|<img src=\"./images/tradeoff.jpeg\" width=\"500\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How do we fall into misrepresentations in machine learning?</h2>\n",
    "\n",
    "\n",
    "$\\Rightarrow$ Can we guarantee capability to identify and quantify the consequences of removing a social-based attribute? \n",
    "\n",
    "$\\Rightarrow$ Who is on the receiving end of these consequences?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The algorithmic frame</h3>\n",
    "\n",
    "Frame centered around choices made when abstracting a problem in the form of representations (input data) and labelling (outcome). \n",
    "\n",
    "Evaluated on accuracy and generalizability to data the model did not train on. \n",
    "\n",
    "**Fairness cannot be defined in this frame $\\Rightarrow$ goal is to produce a model that best captures the relationship between representations and labels.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The data frame</h3>\n",
    "\n",
    "This frame is concerned with the quality of representations (input data) and the outcomes that result. \n",
    "\n",
    "**This additional frame allows us to question the inherent (un)fairness present in input and output data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align='center'>How we frame a problem is key to identifying problems in abstraction</h2>\n",
    "<h3 align='center'>The sociotechnical frame</h3>\n",
    "\n",
    "$$\\text{Technical Component} + \\text{Social Component} = \\text{Sociotechnical system}$$\n",
    "\n",
    "\n",
    "Recognizes that a machine learning model is part of the interaction between people and technology. \n",
    "\n",
    "Any social components of this interaction need to be modelled and incorporated accordingly.\n",
    "\n",
    "**Designers of machine learning systems who fail to consider the way in which social contexts and technologies\n",
    "are interrelated are at risk of falling into \"abstraction traps\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>What traps can we fall into when modeling a social problem?</h2>\n",
    "\n",
    "\n",
    "Machine learning systems used in the real world are inherently sociotechnical\n",
    "systems.\n",
    "\n",
    "Designers of machine learning systems typically translate a real-world context into a machine learning\n",
    "model through abstraction: \n",
    "\n",
    "* Focus on 'relevant' aspects of that context (input, output, relationship)\n",
    "\n",
    "* Problems arise when we abstract away the social context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "\n",
    "Selbst *et al.* [1] identify five traps we can fall into when implementing a machine learning model:\n",
    "\n",
    "* The Solutionism Trap\n",
    "\n",
    "* The Ripple Effect Trap\n",
    "\n",
    "* The Formalism Trap\n",
    "\n",
    "* The Portability Trap\n",
    "\n",
    "* The Framing Trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Solutionism Trap</h3>\n",
    "\n",
    "\n",
    "This trap occurs when we assume that the best solution to a problem\n",
    "may involve technology, and fail to recognize other possible solutions\n",
    "outside of this realm. **Solutionist approaches also fail as definitions of fairness are contested over time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Ripple Effect Trap</h3>\n",
    "\n",
    "This trap occurs when we do not consider the unintended consequences of\n",
    "introducing technology into an existing social system. \n",
    "\n",
    "Changes in behaviors, outcomes, individual experiences, or changes\n",
    "in underlying social values and incentives of a given social system, for\n",
    "instance by increasing perceived value of quantifiable metrics over\n",
    "non-quantifiable ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Formalism Trap</h3>\n",
    "\n",
    "Many tasks of a data scientist involve some form of formalization: from\n",
    "measuring real-world phenomena as data to translating business KPI's\n",
    "and constraints into metrics, loss functions, or parameters. We fall into the\n",
    "formalism trap when we fail to account for the full meaning of social\n",
    "concepts like fairness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Formalism Trap</h3>\n",
    "\n",
    "Fairness is a complex construct that is contested.\n",
    "\n",
    "Mathematical fairness metrics may capture some aspects of fairness, but they\n",
    "fail to capture all relevant aspects. \n",
    "\n",
    "Group fairness metrics do not account for differences in individual experiences nor do they account for procedural justice.\n",
    "\n",
    "Another area in which mathematical abstraction encounters a limitation is when\n",
    "capturing information regarding procedurality, contextuality and contestability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Portability Trap</h3>\n",
    "\n",
    "\n",
    "This trap occurs when we fail to understand how reusing a model or\n",
    "algorithm that is designed for one specific social context may not\n",
    "necessarily apply to a different social context. Reusing an algorithmic\n",
    "solution and failing to take into account differences in involved social\n",
    "contexts can result in misleading results and potentially harmful consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 align='center'>Common traps we can fall into</h2>\n",
    "<h3 align='center'>The Framing Trap</h3>\n",
    "\n",
    "This trap occurs when we fail to consider the full picture surrounding\n",
    "a particular social context when abstracting a social problem. \n",
    "\n",
    "* Social landscape\n",
    "\n",
    "* Characteristics of individuals or circumstances of the chosen situation\n",
    "\n",
    "* Third parties involved along with their circumstances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Introducing FairLearn:<br> a Python library focused on decreasing unfairness in machine learning models </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>About Fairlearn</h2>\n",
    "\n",
    "Fairlearn is an open-source, community-driven project to help data scientists improve fairness of AI systems. It includes:\n",
    "\n",
    "* A Python library for fairness assessment and improvement (fairness metrics, mitigation algorithms, plotting, etc.)\n",
    "\n",
    "* Educational resources covering organizational and technical processes for unfairness mitigation (user guide, case studies, Jupyter notebooks, etc.)\n",
    "\n",
    "The project was started in 2018 at Microsoft Research. In 2021 it adopted neutral governance structure and since then it is completely community-driven.\n",
    "\n",
    "Read more: https://fairlearn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Introduction to the health care scenario</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our scenario builds on previous research that highlighted racial disparities in how health care resources are allocated in the U.S. ([Obermeyer et al., 2019](https://science.sciencemag.org/content/366/6464/447.full)).\n",
    "\n",
    "Motivated by that work, in this tutorial we consider an automated system for recommending patients for _high-risk care management_ programs, which are described by Obermeyer et al. 2019 as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> These programs seek to improve the care of patients with complex health needs by providing additional resources such as greater attention from trained providers, to help ensure that care is well coordinated. \n",
    "\n",
    "> Because the programs are themselves expensive—with costs going toward teams of dedicated nurses, extra primary care appointment slots, and other scarce resources—**health systems rely extensively on algorithms to identify patients who will benefit the most.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Convenience restriction**\n",
    "\n",
    "* In practice, the modeling of health needs would use large data sets covering a wide range of diagnoses. In this tutorial, we will work with a [publicly available clinical dataset](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008) that focuses on _diabetic patients only_ ([Strack et al., 2014](https://www.hindawi.com/journals/bmri/2014/781670/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align='center'>Dataset</h3>\n",
    "\n",
    "* Clincial dataset of hospital re-admissions over a ten-year period (1998-2008) for diabetic patients across 130 different hospitals in the US. \n",
    "\n",
    "* Each record represents the hospital admission records for a patient diagnosed with diabetes whose stay lasted one to fourteen days.\n",
    "\n",
    "* The features include: demographics, diagnoses, diabetic medications, number of visits in the year preceding the encounter, and payer information, whether the patient was readmitted after release, and whether the readmission occurred within 30 days of the release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align='center'>Task</h3>\n",
    "\n",
    "Develop a classification model, which decides whether the patients should be suggested to their primary care physicians for an enrollment into the high-risk care management program. The positive prediction will mean recommendation into the care program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 align='center'>Decision point: Task definition</h3>\n",
    "\n",
    "* A hospital **readmission within 30 days** can be viewed as a proxy that the patients needed more assistance at the release time, so it will be the label we wish to predict.\n",
    "\n",
    "* Because of the class imbalance, we will be measuring our performance via **balanced accuracy**. Another key performance consideration is how many patients are recommended for care, metric we refer to as **selection rate**.\n",
    "\n",
    "Ideally, health care professionals would be involved in both designing and using the model, including formalizing the task definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Fairness considerations**\n",
    "\n",
    "* _Which groups are most likely to be disproportionately negatively affected?_ Previous work suggests that groups with different race and ethnicity can be differently affected.\n",
    "\n",
    "* _What are the harms?_ The key harms here are allocation harms. In particular, false negatives, i.e., don't recommend somebody who will be readmitted.\n",
    "\n",
    "* _How should we measure those harms?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Hands on code section</h2>\n",
    "\n",
    "The key steps in our processing will be (assuming we start with a clean dataset):\n",
    "\n",
    "1. Determine key characteristic that we will use to measure fairness. \n",
    "\n",
    "2. Determine predictive validity.\n",
    "\n",
    "3. Identify imbalances present in data.\n",
    "\n",
    "4. Prepare training and testing data (sklearn).\n",
    "\n",
    "5. Train model (sklearn).\n",
    "\n",
    "6. Evaluate model accuracy.\n",
    "\n",
    "7. Evaluate and adjust fairness of model using Fairlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%run -i process_health_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the perspective of fairness assessment, a key data characteristic is the sample size of groups with respect to which we conduct fairness assessment.\n",
    "\n",
    "Small sample sizes have two implications:\n",
    "\n",
    "* **assessment**: the impacts of the AI system on smaller groups are harder to assess, because due to fewer data points we have a much larger uncertainty (error bars) in our estimates\n",
    "\n",
    "* **model training**: fewer training data points mean that our model fails to appropriately capture any data patterns specific to smaller groups, which means that its predictive performance on these groups could be worse\n",
    "\n",
    "Let's examine the sample sizes of the groups according to `race`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"race\"].value_counts(normalize=True).plot(kind='bar', rot=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"gender\"].value_counts(normalize=True).plot(kind='bar', rot=45); # frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our dataset, our patients are predominantly *Caucasian* (75%). The next largest racial group is *AfricanAmerican*, making up 19% of the patients. The remaining race categories (including *Unknown*) compose only 6% of the data.\n",
    "\n",
    "Gender is in our case effectively binary (and we have no further information how it was operationalized), with both *Female* represented at 54% and *Male* represented at 46%. There are only 3 samples annotated as *Unknown/Invalid*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Please examine the distribution of the `age` feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Predictive validity**\n",
    "\n",
    "We would like to show that our measurement `readmit_30_days` is correlated with patient characteristics that are related to our construct \"benefiting from care management\". One such characteristic is the general patient health, where we expect that patients that are less healthy are more likely to benefit from care management.\n",
    "\n",
    "While our data does not contain full health records that would enable us to holistically measure general patient health, the data does contain two relevant features: `had_emergency` and `had_inpatient_days`, which indicate whether the patient spent any days in the emergency room or in the hospital (but non-emergency) in the preceding year.\n",
    "\n",
    "To establish predictive validity, we would like to show that our measurement `readmit_30_days` is predictive of these two observable characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pointplot(df, \"race\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The patients in the group *Unknown* have a substantially lower rate of emergency visits in the prior year, regardless of whether they are readmitted in 30 days. The readmission is still positively correlated with `had_emergency`, but note the large error bars (due to small sample sizes).\n",
    "\n",
    "We also see that the group with feature value *AfricanAmerican* has a higher rate of emergency visits compared with other groups. However, generally the groups *Caucasian*, *AfricanAmerican* and *Other* follow similar dependence patterns.\n",
    "\n",
    "Again, for *Unknown* the rate of (non-emergency) hospital visits in the previous year is lower than for other groups. In all groups there is a strong positive correlation between `readmit_30_days` and `had_inpatient_days`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "In all cases, we see that readmission in 30 days is predictive of our two measurements of general patient health.\n",
    "\n",
    "The analysis is also surfacing the fact that patients with the value of race *Unknown* have fewer hospital visits in the preceding year (both emergency and non-emergency) than other groups. In practice, this would be a good reason to reach out to health professionals to investigate this patient cohort, to make sure that we understand why there is the systematic difference.\n",
    "\n",
    "Note that we have only investigated _predictive validity_, but there are other important aspects of construct validity which we may want to establish (see [Jacobs and Wallach, 2021](https://arxiv.org/abs/1912.05511))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"exercise-predictive-validity\"></a>\n",
    "### Exercise\n",
    "\n",
    "Check the predictive validity with respect to `gender` and `age`. Do you see any differences? Can you form a hypothesis why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"readmit_30_days\"].value_counts(normalize=True) # frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we can see, the target label is heavily skewed towards the patients not being readmitted within 30 days. In our dataset, only 11% of patients were readmitted within 30 days.\n",
    "\n",
    "Since there are fewer positive examples, we expect that we will have a much larger uncertainty (error bars) in our estimates of *false negative rates* (FNR), compared with *false positive rates* (FPR). This means that there will be larger differences between training FNR and test FNR, even if there is no overfitting, simply because of the smaller sample sizes. \n",
    "\n",
    "Our target metric is *balanced error rate*, which is the average of FPR and FNR. The value of this metric is robust to different frequencies of positives and negatives. However, since half of the metric is contributed by FNR, we expect the uncertainty in balanced error values to behave similarly to the uncertainty of FNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Label imbalance\n",
    "\n",
    "* some classification algorithms and performance measures might not work well with data sets with extreme class imbalance\n",
    "* in binary classification settings, our ability to evaluate error is often driven by the size of the smaller of the two classes (again, the smaller the sample the larger the uncertainty in estimates)\n",
    "* label imbalance may exacerbate the problems due to smaller group sizes in fairness assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's examine how much the label frequencies vary within each group defined by `race`:\n",
    "\n",
    "sns.barplot(x=\"readmit_30_days\", y=\"race\", data=df, ci=95);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see the rate of *30-day readmission* is similar for the *AfricanAmerican* and *Caucasian* groups, but appears smaller for *Other* and smallest for *Unknown* (this is consistent with an overall lower rate of hospital visits in the prior year). The smaller sample size of the *Other* and *Unknown* groups mean that there is more uncertainty around the estimate for these two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prepare training and test datasets\n",
    "\n",
    "As we mentioned in the task definition, our target variable is **readmission within 30 days**, and our sensitive feature for the purposes of fairness assessment is **race**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "[X_train, X_test, Y_train, Y_test, A_train, A_test, df_train, df_test, X] = train_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our performance metric is **balanced accuracy**, so for the purposes of training (but not evaluation!) we will resample the data set, so that it has the same number of positive and negative examples. This means that we can use estimators that optimize standard accuracy (although some estimators allow the use us importance weights).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_bal, Y_train_bal, A_train_bal = resample_dataset(X_train, Y_train, A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Save descriptive statistics of training and test data\n",
    "\n",
    "We next evaluate and save descriptive statistics of the training dataset. These will be provided as part of _model cards_ to document our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_descriptive_stats(A_train_bal, Y_train_bal, A_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model\n",
    "\n",
    "We train a logistic regression model and save its predictions on test data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "unmitigated_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"logistic_regression\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit data\n",
    "unmitigated_pipeline.fit(X_train_bal, Y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "Y_pred_proba = unmitigated_pipeline.predict_proba(X_test)[:,1]\n",
    "Y_pred = unmitigated_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve of probabilistic predictions\n",
    "plot_roc_curve(unmitigated_pipeline, X_test, Y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show balanced accuracy rate of the 0/1 predictions\n",
    "balanced_accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we see, the performance of the model is well above the performance of a coin flip (whose performance would be 0.5 in both cases), albeit it is quite far from a perfect classifier (whose performance would be 1.0 in both cases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspect the coefficients of trained model\n",
    "\n",
    "We check the coefficients of the fitted model to make sure that they \"makes sense\". While subjective, this step is important and helps catch mistakes and might point out to some fairness issues. However, we will systematically assess the fairness of the model in the next section.\n",
    "\n",
    "Coefficients represent the model's log-odds. Doubling a categorical variable, or increasing a continuous variable by a standard deviation, doubles the odds of the classifier going towards the “1” case. This is a true statistical measure (according to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coef_series = pd.Series(data=unmitigated_pipeline.named_steps[\"logistic_regression\"].coef_[0], index=X.columns)\n",
    "coef_series.sort_values().plot.barh(figsize=(4, 12), legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fairness assessment with `MetricFrame`\n",
    "\n",
    "Fairlearn provides the data structure called `MetricFrame` to enable evaluation of disaggregated metrics. We will show how to use a `MetricFrame` object to assess the trained `LogisticRegression` classifier for potential fairness-related harms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In its simplest form MetricFrame takes four arguments:\n",
    "#    metric_function with signature metric_function(y_true, y_pred)\n",
    "#    y_true: array of labels\n",
    "#    y_pred: array of predictions\n",
    "#    sensitive_features: array of sensitive feature values\n",
    "\n",
    "mf1 = MetricFrame(metrics=false_negative_rate,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])\n",
    "\n",
    "# The disaggregated metrics are stored in a pandas Series mf1.by_group:\n",
    "\n",
    "mf1.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio and worst-case performance are accessed as\n",
    "#   mf1.difference(), mf1.ratio(), mf1.group_max()\n",
    "\n",
    "print(f\"difference: {mf1.difference():.3}\\n\"\n",
    "      f\"ratio: {mf1.ratio():.3}\\n\"\n",
    "      f\"max across groups: {mf1.group_max():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You can also evaluate multiple metrics by providing a dictionary\n",
    "\n",
    "metrics_dict = {\n",
    "    \"selection_rate\": selection_rate,\n",
    "    \"false_negative_rate\": false_negative_rate,\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,\n",
    "}\n",
    "\n",
    "metricframe_unmitigated = MetricFrame(metrics=metrics_dict,\n",
    "                  y_true=Y_test,\n",
    "                  y_pred=Y_pred,\n",
    "                  sensitive_features=df_test['race'])\n",
    "\n",
    "# The disaggregated metrics are then stored in a pandas DataFrame:\n",
    "\n",
    "metricframe_unmitigated.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The largest difference, smallest ratio, and the maximum and minimum values\n",
    "# across the groups are then all pandas Series, for example:\n",
    "\n",
    "metricframe_unmitigated.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You'll probably want to view them transposed:\n",
    "pd.DataFrame({'difference': metricframe_unmitigated.difference(),\n",
    "              'ratio': metricframe_unmitigated.ratio(),\n",
    "              'group_min': metricframe_unmitigated.group_min(),\n",
    "              'group_max': metricframe_unmitigated.group_max()}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You can also easily plot all of the metrics using DataFrame plotting capabilities\n",
    "\n",
    "metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
    "                      legend=False, rot=-45, position=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "According to the above bar chart, it seems that the group *Unknown* is selected for the care management program less often than other groups as reflected by the selection rate. Also this group experiences the largest false negative rate, so a larger fraction of group members that are likely to benefit from the care management program are not selected. Finally, the balanced accuracy on this group is also the lowest.\n",
    "\n",
    "\n",
    "We observe disparity, even though we did not include race in our model. There's a variety of reasons why such disparities may occur. It could be due to representational issues (i.e., not enough instances per group), or because the feature distribution itself differs across groups (i.e., different relationship between features and target variable, obvious example would be people with darker skin in facial recognition systems, but can be much more subtle). Real-world applications often exhibit both kinds of issues at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Postprocessing with `ThresholdOptimizer`\n",
    "\n",
    "**Postprocessing** techniques are a class of unfairness-mitigation algorithms that take an already trained model and a dataset as an input and seek to fit a transformation function to model's outputs to satisfy some (group) fairness constraint(s). They might be the only feasible unfairness mitigation approach when developers cannot influence training of the model, due to practical reasons or due to security or privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we use the `ThresholdOptimizer` algorithm from Fairlearn, which follows the approach of [Hardt, Price, and Srebro (2016)](https://arxiv.org/abs/1610.02413).\n",
    "\n",
    "`ThresholdOptimizer` takes in an exisiting (possibly pre-fit) machine learning model whose predictions act as a scoring function and identifies a separate thrceshold for each group in order to optimize some specified objective metric (such as **balanced accuracy**) subject to specified fairness constraints (such as **false negative rate parity**). Thus, the resulting classifier is just a suitably thresholded version of the underlying machinelearning model.\n",
    "\n",
    "The constraint **false negative rate parity** requires that all the groups have equal values of false negative rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To instatiate our `ThresholdOptimizer`, we pass in:\n",
    "\n",
    "*   An existing `estimator` that we wish to threshold. \n",
    "*   The fairness `constraints` we want to satisfy.\n",
    "*   The `objective` metric we want to maximize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we instantite ThresholdOptimizer with the logistic regression estimator\n",
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=unmitigated_pipeline,\n",
    "    constraints=\"false_negative_rate_parity\",\n",
    "    objective=\"balanced_accuracy_score\",\n",
    "    prefit=True,\n",
    "    predict_method='predict_proba'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to use the `ThresholdOptimizer`, we need access to the sensitive features **both during training time and once it's deployed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "postprocess_est.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Record and evaluate the output of the trained ThresholdOptimizer on test data\n",
    "\n",
    "Y_pred_postprocess = postprocess_est.predict(X_test, sensitive_features=A_test)\n",
    "metricframe_postprocess = MetricFrame(\n",
    "    metrics=metrics_dict,\n",
    "    y_true=Y_test,\n",
    "    y_pred=Y_pred_postprocess,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.by_group,\n",
    "           metricframe_postprocess.by_group],\n",
    "           keys=['Unmitigated', 'ThresholdOptimizer'],\n",
    "           axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([metricframe_unmitigated.difference(),\n",
    "           metricframe_postprocess.difference()],\n",
    "          keys=['Unmitigated: difference', 'ThresholdOptimizer: difference'],\n",
    "          axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "metricframe_postprocess.by_group.plot.bar(subplots=True, layout=[1,3], figsize=(12, 4), legend=False, rot=-45, position=1.5)\n",
    "postprocess_performance = figure_to_base64str(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align='center'>Final remarks</h2>\n",
    "\n",
    "In this workshop we motivated the importance of incorporating and assessing fairness metrics as part of a problem that uses machine learning techniques. \n",
    "\n",
    "We covered an overview of what abstraction is, how harms in abstraction take place, and the different traps we can fall into when our abstraction does not incorporate social context. \n",
    "\n",
    "We applied what we learned to a scenario involving patient enrollment in high-risk care management program. We explored the Fairlearn Python library to improve unfair outcomes towards patients whose race was unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Future readings and exercises:\n",
    "\n",
    "You can learn more about Fairlearn https://fairlearn.org/v0.7.0/about/index.html and visit their open source code https://github.com/fairlearn/fairlearn\n",
    "\n",
    "Fairlearn is a community-based effort to improve ethics in AI. \n",
    "\n",
    "Curious to contribute? Visit https://fairlearn.org/v0.7.0/contributor_guide/index.html "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "rise": {
   "autolaunch": false,
   "backimage": "Cybera_contact.svg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
